{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe purpose of this code is to classify legal text by type.\\n\\nTypes include:\\n1.) Complaint\\n2.) Order\\n3.) Summary judgement\\n4.) Cover sheet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Part II Legal Doc Classification \n",
    "\n",
    "Part I was the concatenating and cleaning of the Text docs. \n",
    "\n",
    "Part II\n",
    "\n",
    "The purpose of this code is to create a frequency distribution of words aggregated by legal document type.  The words \n",
    "in each file are compared to a unique set of words created from all word documents.   These frequency dist can then \n",
    "be used to classify text by looking for differences amongst the text. \n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-52df0cb697c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhello\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hello'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TARGET DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note:    The target directory should be changed by the user to point to the directory within which they have saved their\n",
    "            text files'''\n",
    "\n",
    "os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "Dir_list = os.listdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR TEXT CLEARNING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text_4_classification_remove_backslashes(Text_file):\n",
    "    '''The purpose of this function is to clean the text files of numerous instances of backslashes \n",
    "    in order to prepare them for the regex expression search. \n",
    "    Input  =   Single text file \n",
    "    Output =   Single text file cleaned \n",
    "    '''\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    Text_file_lower = Text_file.lower()\n",
    "    \n",
    "    # Split any values in the text on the backslash.  The Text_split_slash should return a list. \n",
    "    Text_split_slash = Text_file_lower.split('\\\\')\n",
    "        \n",
    "    # Return the list to a text. \n",
    "    Text_rejoined = ' '.join(Text_split_slash)\n",
    "                \n",
    "    # Return a list of the cleaned text docs. \n",
    "    return Text_rejoined\n",
    "\n",
    "\n",
    "def clean_text_4_classification_remove_nABC(Text_file):\n",
    "    '''The purpose of this function is to remove the 'n' that appears before words that begin with an upper case letter.  \n",
    "    Input  =   Single txt file\n",
    "    Output =   Clean list of tokens from original txt file\n",
    "    '''\n",
    "    # Define the regex expression that you want to search for. \n",
    "    Regex_exp = re.compile('n[A-Z*]')\n",
    "    \n",
    "    # Create a list to capture the tokens once they are cleaned \n",
    "    Text_tokenized_cleaned = []\n",
    "            \n",
    "    # Tokenize the given text\n",
    "    Text_tokenized = nltk.word_tokenize(Text_file)\n",
    "            \n",
    "    # Run for loop over tokens for a given text. \n",
    "    for token in Text_tokenized:\n",
    "\n",
    "        # Search for the regex expression\n",
    "        Regex_search = re.search(Regex_exp, token)\n",
    "                \n",
    "        # Test if there was match (None = no match)\n",
    "        if Regex_search != None:\n",
    "                     \n",
    "            # If there was a match, take all letters after the 'n'.   \n",
    "            token_cleaned = token[1:]\n",
    "                    \n",
    "            Text_tokenized_cleaned.append(token_cleaned)\n",
    "                        \n",
    "        # If the Regex_search returned None, return the token back to the Text_tokenized_cleaned list\n",
    "        else:\n",
    "            Text_tokenized_cleaned.append(token)\n",
    "    \n",
    "    # Return a list of clean tokens\n",
    "    return Text_tokenized_cleaned\n",
    "\n",
    "\n",
    "def create_dict_punct():\n",
    "    '''The purpose of this function is to simply create a dictionary of punctuation symbols to use\n",
    "    in other functions\n",
    "    Input  = None\n",
    "    Output = Dict whose keys are the distinct punctuation marks. \n",
    "    '''\n",
    "    import string\n",
    "    Dict = {}\n",
    "    Punct = string.punctuation\n",
    "    for x in Punct:\n",
    "        Dict[x] = ''\n",
    "    return Dict \n",
    "\n",
    "def strip_punctuation(Token_list):\n",
    "    '''The purpose of this function is to strip the punctuation from a list of tokens. \n",
    "    Input  =  List of tokens\n",
    "    Output =  List of tokens absent punctuation.  \n",
    "    '''\n",
    "    # Import punctuation dictionary\n",
    "    Dict_punct = create_dict_punct()\n",
    "\n",
    "    # Create a list to capture the cleaned tokens\n",
    "    Clean_token_list = []    \n",
    "        \n",
    "    # Iterate over the tokens in the txt file\n",
    "    for x in Token_list:\n",
    "        if x not in Dict_punct:\n",
    "            # Append tokens to clean token list\n",
    "            Clean_token_list.append(x)\n",
    "    \n",
    "    # Return a list of cleaned text\n",
    "    return Clean_token_list\n",
    "\n",
    "def strip_two_letter_words(Token_list):\n",
    "    '''The purpose of this function is to remove any two letter tokens from a list of tokens.\n",
    "    Input  =   List of tokens\n",
    "    Output =   List of tokens absent two letter words'''\n",
    "    \n",
    "    List = [x for x in Token_list if len(x) > 2]\n",
    "    \n",
    "    return List\n",
    "\n",
    "def create_dict_stopwords():\n",
    "    '''The purpose of this code is to create a dictionary of stop words. \n",
    "    Input  = None\n",
    "    Output = Dictionary of stop words'''\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    Stopwords = stopwords.words('english')                  \n",
    "    Dict = {}\n",
    "    for x in Stopwords:\n",
    "        Dict[x] = ''\n",
    "    return Dict\n",
    "\n",
    "def strip_stop_words(Token_list):\n",
    "    ''' The purpose of this code is to strip the stop words from a given text\n",
    "    Input  = List of tokens \n",
    "    Outpu  = Text clean of stop words'''\n",
    "    \n",
    "    stop_words = create_dict_stopwords()\n",
    "    List = []\n",
    "    for x in Token_list:\n",
    "        if x not in stop_words:\n",
    "            List.append(x)\n",
    "    return List\n",
    "\n",
    "def create_Concatenated_text_file(Dir_list, New_file_name):     \n",
    "    # Create new write file\n",
    "    New_File = open(str(New_file_name) + '.txt','w')\n",
    "    \n",
    "    # Identify text files to retreive\n",
    "    Text_files = (file for file in Dir_list if 'txt.' in file)  # attempt to use a generator. \n",
    "\n",
    "    # Create Loop Through List of Directories\n",
    "    for x in Text_files:\n",
    "        File = open(x, 'rb')\n",
    "        Text = File.read()\n",
    "        \n",
    "        # Write files to new file\n",
    "        New_File.write(str(Text))\n",
    "        New_File.write('\\n')\n",
    "    # Close File\n",
    "    New_File.close()\n",
    "\n",
    "def write_to_text_file(Text_2_write, File_name2_use):\n",
    "    file = open(str(File_name2_use) + '.txt', 'w')    \n",
    "    file.write(Text_2_write)   \n",
    "\n",
    "    \n",
    "def get_cleaned_concatenated_text_file(Dir_list):\n",
    "    '''\n",
    "    Input  = List of files in the directory\n",
    "    Output = Cleaned text \n",
    "    \n",
    "    '''\n",
    "    # Note, the author assumes there is only one Concat file in the dir.  Since the order of the files in the dir\n",
    "    # can change, the better approach is to identify it using a list comprehension with an if statement. \n",
    "    Clean_concat_text = [file for file in Dir_list if 'Concatenated Text File - Cleaned' in file]\n",
    "    \n",
    "    for text_file in Clean_concat_text:\n",
    "        \n",
    "        File = open(text_file)\n",
    "        # Read in dirty text\n",
    "        Text_dirty = File.read()\n",
    "        # Run cleaning pipeline\n",
    "        Clean_text = text_clearning_pipeline_Input_4_Error_Checker_Function(Text_dirty)\n",
    "    \n",
    "        # Return cleaned text\n",
    "    return Clean_text\n",
    "\n",
    "\n",
    "def create_Wordnet_set():\n",
    "    '''The purpose of this function is to create a set of all words from the wordnet dictionary.\n",
    "    Input  = None\n",
    "    Output = Set object of all words. \n",
    "    '''\n",
    "    # Import words from wordnet\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    Words = wn.words()\n",
    "\n",
    "    # Create List to capture words  \n",
    "    List_dict_words = []; [List_dict_words.append(x) for x in Words]\n",
    "    \n",
    "    # Create Set\n",
    "    Set_dict_words = set(List_dict_words)\n",
    "    \n",
    "    # Return Set\n",
    "    return Set_dict_words\n",
    "\n",
    "def correct_tokens_nABC_using_wordnet_dict(Token_list):\n",
    "    '''The purpose of this code is to '''\n",
    "    \n",
    "    Wordnet_set = create_Wordnet_set()\n",
    "    \n",
    "    # Creat a clean list of tokens to return to the user. \n",
    "    Token_list_cleaned = []\n",
    "    \n",
    "    # Convert tokens to lowercase\n",
    "    Token_list_lower = [x.lower() for x in Token_list]\n",
    "    \n",
    "    # Loop over the list of tokens\n",
    "    for token in Token_list_lower:\n",
    "        # Find the tokens that start with an 'n'\n",
    "        if token[0] == 'n':\n",
    "            # See if the token is in the WordNet Dict when the 'n' is dropped\n",
    "            if token[1:] in Wordnet_set:\n",
    "                # If the token is in the dictionary, append the token without the 'n'\n",
    "                Token_list_cleaned.append(token[1:])\n",
    "            else:\n",
    "                # If not, then just append the token as there was no matching word. \n",
    "                Token_list_cleaned.append(token)\n",
    "                \n",
    "        # If the token does not start with an 'n', then this code does not apply and append back to the list. \n",
    "        else:\n",
    "            Token_list_cleaned.append(token)\n",
    "    \n",
    "    return Token_list_cleaned\n",
    "\n",
    "def get_isalpha(Token_list):\n",
    "    List = [x for x in Token_list if x.isalpha()]\n",
    "    return List\n",
    "\n",
    "def get_set_from_text(Dir_list):\n",
    "    '''The purpose of this code is to create a set of unique tokens from a text file as a string object. \n",
    "    Input  =  Text file as a string object \n",
    "    Output =  Set of unique tokens. \n",
    "    '''\n",
    "    \n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    Dir_list = os.listdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    \n",
    "    # Define Set Object\n",
    "    Create_set = ''\n",
    "    \n",
    "    # Obtain Your Target File\n",
    "    Target_file = [file for file in Dir_list if 'Cleaned' in file]\n",
    "    \n",
    "    # Loop over Target_file since it is a generator object. \n",
    "    for file in Target_file:\n",
    "        File = open(file)\n",
    "        Text = File.read()\n",
    "        # Tokenize Text\n",
    "        Text_tokenized = nltk.word_tokenize(Text)\n",
    "        # Create Set\n",
    "        Create_set = set(Text_tokenized)\n",
    "    # Return Set\n",
    "    return Create_set\n",
    "\n",
    "def get_clean_text_using_text_clearning_pipeline(Text_file):\n",
    "    '''The purpose of this function is to prepare text for use with the Error Checker Program\n",
    "    Input  =  Single text file\n",
    "    Output =  List of clean tokens representing a single text. \n",
    "    '''\n",
    "    # Run Clearning Pipeline (These functions are taken from the ones define above)\n",
    "    \n",
    "    # Input Text Files\n",
    "    txt_strip_backslashes = clean_text_4_classification_remove_backslashes(Text_file)\n",
    "    txt_strip_nABC = clean_text_4_classification_remove_nABC(txt_strip_backslashes)\n",
    "    # Input Tokens - **At a later time you can loop over a single list of tokens and run them through these functions. \n",
    "    # Then remove the list creation from each function.  This should spead up the function markedly. \n",
    "    txt_strip_punct = strip_punctuation(txt_strip_nABC)\n",
    "    txt_strip_2_letter_words = strip_two_letter_words(txt_strip_punct)\n",
    "    txt_strip_stop_words = strip_stop_words(txt_strip_2_letter_words)\n",
    "    txt_correct_nABC_using_wordnet = correct_tokens_nABC_using_wordnet_dict(txt_strip_stop_words)\n",
    "    Final_tokenized_text = get_isalpha(txt_correct_nABC_using_wordnet)\n",
    "    \n",
    "    return Final_tokenized_text\n",
    "\n",
    "def create_dataframe_first_N_num_txt_files_target_dir():\n",
    "    # Create Dataframe 1st Hundred Txt Files\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    Dir_list = os.listdir()\n",
    "    Text_files = [x for x in Dir_list if '.txt' in x]\n",
    "    List_hund = Text_files[3:103]\n",
    "    \n",
    "    df = pd.DataFrame(List_hund)\n",
    "    \n",
    "    write_to_excel(df, 'Classify 100 Text Files')\n",
    "    \n",
    "    return None\n",
    "    \n",
    "def get_list_uniqueTokens_from_cleaned_concat_text():\n",
    "    # Get List Unique Tokens From Cleaned Concatenated Text\n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    File = 'Concatenated Text File - Cleaned.txt'\n",
    "    Open = open(File, 'rb')\n",
    "    Read = Open.read()\n",
    "    Str = str(Read)\n",
    "    Tokenize = nltk.word_tokenize(Str)    \n",
    "    Set = set(Tokenize)\n",
    "    List = list(Set)\n",
    "\n",
    "    return List\n",
    "\n",
    "def write_to_excel(dataframe, filename):\n",
    "    import pandas as pd\n",
    "    writer = pd.ExcelWriter(filename+'.xlsx')\n",
    "    dataframe.to_excel(writer, sheet_name = 'Data')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get List Unique Tokens From Cleaned Concatenated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_uniqueTokens = get_list_uniqueTokens_from_cleaned_concat_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['federalgovemment',\n",
       " 'pittsburgh',\n",
       " 'insure',\n",
       " 'ninvestigated',\n",
       " 'gilbert',\n",
       " 'voted',\n",
       " 'qmmuacwmmnmumnm',\n",
       " 'premises',\n",
       " 'hrchitect',\n",
       " 'disabihty',\n",
       " 'nwalters',\n",
       " 'rested',\n",
       " 'nquestions',\n",
       " 'dominican',\n",
       " 'ausmzss',\n",
       " 'hug',\n",
       " 'squadron',\n",
       " 'wtormsaction',\n",
       " 'diminish',\n",
       " 'bxismnccufhighly',\n",
       " 'anela',\n",
       " 'congressman',\n",
       " 'lojst',\n",
       " 'treuble',\n",
       " 'retailiated',\n",
       " 'puppy',\n",
       " 'rowe',\n",
       " 'fulltime',\n",
       " 'clusion',\n",
       " 'plaintiffupon',\n",
       " 'mwu',\n",
       " 'manager',\n",
       " 'nshelton',\n",
       " 'perverse',\n",
       " 'nsqcq',\n",
       " 'skemp',\n",
       " 'wwwee',\n",
       " 'mggg',\n",
       " 'mwmgme',\n",
       " 'pmcl',\n",
       " 'mrwmawrw',\n",
       " 'suffice',\n",
       " 'ngallivan',\n",
       " 'kent',\n",
       " 'primafacz',\n",
       " 'tests',\n",
       " 'cannibal',\n",
       " 'ahtain',\n",
       " 'accominodations',\n",
       " 'distantly',\n",
       " 'nmbmw',\n",
       " 'entice',\n",
       " 'muching',\n",
       " 'affected',\n",
       " 'propertymcarrying',\n",
       " 'nenforcers',\n",
       " 'njobsite',\n",
       " 'endurance',\n",
       " 'relocating',\n",
       " 'nchayanne',\n",
       " 'prepared',\n",
       " 'nchastain',\n",
       " 'thorn',\n",
       " 'pennsyivanfa',\n",
       " 'bone',\n",
       " 'dsrzald',\n",
       " 'xabossw',\n",
       " 'rapping',\n",
       " 'dylesha',\n",
       " 'disseminated',\n",
       " 'disqvery',\n",
       " 'vuong',\n",
       " 'ndoleman',\n",
       " 'onesboro',\n",
       " 'begner',\n",
       " 'nperpetrators',\n",
       " 'eor',\n",
       " 'rescind',\n",
       " 'altamonte',\n",
       " 'nchiefs',\n",
       " 'inform',\n",
       " 'neeec',\n",
       " 'farmer',\n",
       " 'trim',\n",
       " 'aqem',\n",
       " 'nenj',\n",
       " 'wwhite',\n",
       " 'ndebra',\n",
       " 'nsuperiors',\n",
       " 'shawhib',\n",
       " 'atlantic',\n",
       " 'commodti',\n",
       " 'spun',\n",
       " 'scored',\n",
       " 'udicial',\n",
       " 'nconfronted',\n",
       " 'waajid',\n",
       " 'excludes',\n",
       " 'nvaleri',\n",
       " 'zohourys',\n",
       " 'bani',\n",
       " 'wiped',\n",
       " 'mishandling',\n",
       " 'nexcludable',\n",
       " 'controverts',\n",
       " 'segtember',\n",
       " 'geople',\n",
       " 'aff',\n",
       " 'npurposeof',\n",
       " 'rmvdkc',\n",
       " 'horst',\n",
       " 'nenclosuresq',\n",
       " 'pre',\n",
       " 'weekend',\n",
       " 'ndivisihn',\n",
       " 'esparza',\n",
       " 'allegations',\n",
       " 'camara',\n",
       " 'nonmovant',\n",
       " 'hvn',\n",
       " 'nluton',\n",
       " 'suburbs',\n",
       " 'nelsaamrosmsommymmr',\n",
       " 'dirt',\n",
       " 'theretd',\n",
       " 'entering',\n",
       " 'electrician',\n",
       " 'instrumentalities',\n",
       " 'lengthy',\n",
       " 'awed',\n",
       " 'meneses',\n",
       " 'vloiate',\n",
       " 'afpeal',\n",
       " 'credibility',\n",
       " 'camousvwncrou',\n",
       " 'ndettmer',\n",
       " 'nimim',\n",
       " 'remediate',\n",
       " 'publicize',\n",
       " 'severat',\n",
       " 'sail',\n",
       " 'nelsssmsouooxnnmmm',\n",
       " 'disrimination',\n",
       " 'hips',\n",
       " 'sevier',\n",
       " 'negligible',\n",
       " 'crowe',\n",
       " 'leadership',\n",
       " 'prime',\n",
       " 'package',\n",
       " 'foir',\n",
       " 'postings',\n",
       " 'issuesare',\n",
       " 'machine',\n",
       " 'ssdi',\n",
       " 'krutzig',\n",
       " 'meetin',\n",
       " 'correlated',\n",
       " 'naed',\n",
       " 'nsharonda',\n",
       " 'nahgarber',\n",
       " 'stole',\n",
       " 'proprietor',\n",
       " 'ptaintiff',\n",
       " 'midtown',\n",
       " 'ngpursuant',\n",
       " 'nmadeline',\n",
       " 'congressional',\n",
       " 'theyene',\n",
       " 'populated',\n",
       " 'nawalter',\n",
       " 'bootscase',\n",
       " 'taking',\n",
       " 'onyx',\n",
       " 'nrejecting',\n",
       " 'hahlcms',\n",
       " 'nga',\n",
       " 'mmwmommmwmmmxmnnm',\n",
       " 'enforceable',\n",
       " 'honor',\n",
       " 'retorted',\n",
       " 'pooped',\n",
       " 'nevoting',\n",
       " 'ndmmmusaonm',\n",
       " 'auto',\n",
       " 'ninvited',\n",
       " 'notlco',\n",
       " 'visit',\n",
       " 'idefendant',\n",
       " 'njumsmcrmnu',\n",
       " 'nversions',\n",
       " 'andreia',\n",
       " 'nshould',\n",
       " 'mnt',\n",
       " 'nkathy',\n",
       " 'unbeknown',\n",
       " 'suffered',\n",
       " 'peak',\n",
       " 'liam',\n",
       " 'nsubmissions',\n",
       " 'bel',\n",
       " 'persuasion',\n",
       " 'slap',\n",
       " 'fmal',\n",
       " 'murder',\n",
       " 'efnotice',\n",
       " 'proposing',\n",
       " 'nlightsey',\n",
       " 'graduates',\n",
       " 'ceased',\n",
       " 'ncomplains',\n",
       " 'ineiuding',\n",
       " 'extcndad',\n",
       " 'enrich',\n",
       " 'drk',\n",
       " 'truthfulness',\n",
       " 'loggerhead',\n",
       " 'nmotions',\n",
       " 'wmgmmmmmawammg',\n",
       " 'nvictims',\n",
       " 'writs',\n",
       " 'discriminationcase',\n",
       " 'congregational',\n",
       " 'anchor',\n",
       " 'exotic',\n",
       " 'brewer',\n",
       " 'trading',\n",
       " 'document',\n",
       " 'apptwepriate',\n",
       " 'nnial',\n",
       " 'lose',\n",
       " 'nayalla',\n",
       " 'trutv',\n",
       " 'nsuitors',\n",
       " 'foice',\n",
       " 'suggesting',\n",
       " 'wht',\n",
       " 'mscnalw',\n",
       " 'samuel',\n",
       " 'comma',\n",
       " 'mirrors',\n",
       " 'eeisntinuee',\n",
       " 'boot',\n",
       " 'stanley',\n",
       " 'casksb',\n",
       " 'rehired',\n",
       " 'whi',\n",
       " 'clientele',\n",
       " 'hale',\n",
       " 'recdvbiy',\n",
       " 'talk',\n",
       " 'invitationcase',\n",
       " 'plainticc',\n",
       " 'alonso',\n",
       " 'raquind',\n",
       " 'ncal',\n",
       " 'membership',\n",
       " 'nfer',\n",
       " 'critiqued',\n",
       " 'lindsey',\n",
       " 'dispatch',\n",
       " 'rulecase',\n",
       " 'mpg',\n",
       " 'factors',\n",
       " 'functieas',\n",
       " 'ormmmonmnsamonmwmmmmnmm',\n",
       " 'timeclock',\n",
       " 'lock',\n",
       " 'plaintiffpertormed',\n",
       " 'nsumming',\n",
       " 'disseminating',\n",
       " 'nsachs',\n",
       " 'nually',\n",
       " 'raped',\n",
       " 'purview',\n",
       " 'ejc',\n",
       " 'theme',\n",
       " 'ndecorators',\n",
       " 'motivated',\n",
       " 'pine',\n",
       " 'jamison',\n",
       " 'planning',\n",
       " 'ratification',\n",
       " 'nwhaalleges',\n",
       " 'longstanding',\n",
       " 'nstricter',\n",
       " 'nmanpowergroup',\n",
       " 'nreqaest',\n",
       " 'spreading',\n",
       " 'ndocamentation',\n",
       " 'ual',\n",
       " 'lint',\n",
       " 'theological',\n",
       " 'nmsbm',\n",
       " 'going',\n",
       " 'competing',\n",
       " 'nquinisha',\n",
       " 'foyce',\n",
       " 'carried',\n",
       " 'oster',\n",
       " 'key',\n",
       " 'notrequired',\n",
       " 'outweigh',\n",
       " 'questionnaire',\n",
       " 'mormmsnmg',\n",
       " 'injunc',\n",
       " 'ndsfendant',\n",
       " 'postponed',\n",
       " 'nkani',\n",
       " 'underlay',\n",
       " 'haw',\n",
       " 'geneticinformation',\n",
       " 'nbetancur',\n",
       " 'usgovernment',\n",
       " 'evan',\n",
       " 'directar',\n",
       " 'neiocdpyrigeis',\n",
       " 'othemlse',\n",
       " 'metzler',\n",
       " 'nmstoetww',\n",
       " 'measuring',\n",
       " 'reserve',\n",
       " 'grouts',\n",
       " 'employmentcase',\n",
       " 'mmmmon',\n",
       " 'conclusor',\n",
       " 'regaipt',\n",
       " 'nmmmmmmr',\n",
       " 'protracted',\n",
       " 'provisionally',\n",
       " 'nmethods',\n",
       " 'quincy',\n",
       " 'interferes',\n",
       " 'greeted',\n",
       " 'premeditated',\n",
       " 'game',\n",
       " 'see',\n",
       " 'fertilization',\n",
       " 'wit',\n",
       " 'neglect',\n",
       " 'dionna',\n",
       " 'nnorthlake',\n",
       " 'threughout',\n",
       " 'measured',\n",
       " 'nusblqabanta',\n",
       " 'ela',\n",
       " 'actioes',\n",
       " 'nreinsured',\n",
       " 'sadie',\n",
       " 'enrichment',\n",
       " 'elvpcjrtiomejjt',\n",
       " 'cablfjsateliji',\n",
       " 'iaborimgmt',\n",
       " 'fox',\n",
       " 'charts',\n",
       " 'diligent',\n",
       " 'hollering',\n",
       " 'fernando',\n",
       " 'destroyed',\n",
       " 'requester',\n",
       " 'voorhis',\n",
       " 'shedid',\n",
       " 'smeared',\n",
       " 'mmmmmmmmmmnm',\n",
       " 'disregarded',\n",
       " 'basecl',\n",
       " 'nowns',\n",
       " 'nresponding',\n",
       " 'muma',\n",
       " 'nmultidifi',\n",
       " 'nlmpol',\n",
       " 'unilaterally',\n",
       " 'sav',\n",
       " 'testifies',\n",
       " 'moora',\n",
       " 'wareheuse',\n",
       " 'engendered',\n",
       " 'gribia',\n",
       " 'shellac',\n",
       " 'defsndant',\n",
       " 'discriniination',\n",
       " 'septemberl',\n",
       " 'nmmmmcr',\n",
       " 'reedfordiscovmrami',\n",
       " 'like',\n",
       " 'copious',\n",
       " 'xaegmaitcom',\n",
       " 'ndoes',\n",
       " 'nellenwaod',\n",
       " 'delving',\n",
       " 'ostracizing',\n",
       " 'nagrees',\n",
       " 'modi',\n",
       " 'conciliatien',\n",
       " 'banned',\n",
       " 'ballard',\n",
       " 'pwing',\n",
       " 'just',\n",
       " 'sheriffs',\n",
       " 'nmonica',\n",
       " 'npriorto',\n",
       " 'requisite',\n",
       " 'chano',\n",
       " 'essentially',\n",
       " 'subordinates',\n",
       " 'releted',\n",
       " 'nmegdrichardsen',\n",
       " 'celebration',\n",
       " 'kids',\n",
       " 'neiioinsurance',\n",
       " 'iniicate',\n",
       " 'hinge',\n",
       " 'nplatforms',\n",
       " 'soles',\n",
       " 'nlt',\n",
       " 'willcase',\n",
       " 'mmusw',\n",
       " 'nmmmmwmm',\n",
       " 'commuting',\n",
       " 'lunchrooms',\n",
       " 'ford',\n",
       " 'review',\n",
       " 'affirming',\n",
       " 'bangladeshi',\n",
       " 'mcase',\n",
       " 'ended',\n",
       " 'nngm',\n",
       " 'subparts',\n",
       " 'latin',\n",
       " 'homosexual',\n",
       " 'fl',\n",
       " 'bib',\n",
       " 'nsubstitutes',\n",
       " 'participated',\n",
       " 'ram',\n",
       " 'unconditionally',\n",
       " 'sonduct',\n",
       " 'ofd',\n",
       " 'pannell',\n",
       " 'expound',\n",
       " 'nresidents',\n",
       " 'witheomplaints',\n",
       " 'wtcraig',\n",
       " 'noti',\n",
       " 'willingly',\n",
       " 'completed',\n",
       " 'potashnick',\n",
       " 'wv',\n",
       " 'menu',\n",
       " 'avp',\n",
       " 'acf',\n",
       " 'incontinence',\n",
       " 'broadnax',\n",
       " 'bedrock',\n",
       " 'enteringcase',\n",
       " 'stellar',\n",
       " 'pilots',\n",
       " 'plaitniff',\n",
       " 'sewell',\n",
       " 'equa',\n",
       " 'iiicase',\n",
       " 'hospitality',\n",
       " 'airlines',\n",
       " 'abilityto',\n",
       " 'disorder',\n",
       " 'mention',\n",
       " 'jim',\n",
       " 'wifciassaqi',\n",
       " 'ndireci',\n",
       " 'gpvmm',\n",
       " 'nmcdonald',\n",
       " 'nmmmnme',\n",
       " 'ndeman',\n",
       " 'interpreters',\n",
       " 'legra',\n",
       " 'alp',\n",
       " 'facts',\n",
       " 'efficiently',\n",
       " 'kristi',\n",
       " 'eligibility',\n",
       " 'snipes',\n",
       " 'nelse',\n",
       " 'bliss',\n",
       " 'bonus',\n",
       " 'sadiga',\n",
       " 'chebotnikov',\n",
       " 'nhdddd',\n",
       " 'ndudwmtivewacii',\n",
       " 'nearly',\n",
       " 'unloaded',\n",
       " 'orthern',\n",
       " 'digital',\n",
       " 'belongings',\n",
       " 'john',\n",
       " 'pharm',\n",
       " 'ncomission',\n",
       " 'laborer',\n",
       " 'emotional',\n",
       " 'along',\n",
       " 'standmds',\n",
       " 'medications',\n",
       " 'penalty',\n",
       " 'joins',\n",
       " 'utterly',\n",
       " 'ics',\n",
       " 'olsen',\n",
       " 'pulling',\n",
       " 'gchat',\n",
       " 'delisha',\n",
       " 'nenterprises',\n",
       " 'mableton',\n",
       " 'auctlt',\n",
       " 'camiceria',\n",
       " 'appellantgs',\n",
       " 'imaging',\n",
       " 'wwqwwwwwwwwwwww',\n",
       " 'eppropriate',\n",
       " 'reassigning',\n",
       " 'portunity',\n",
       " 'gccx',\n",
       " 'wall',\n",
       " 'kmosks',\n",
       " 'nthreats',\n",
       " 'ghengis',\n",
       " 'ngladys',\n",
       " 'galeb',\n",
       " 'nmmmmm',\n",
       " 'strh',\n",
       " 'perttal',\n",
       " 'restmg',\n",
       " 'disequilibrium',\n",
       " 'delicase',\n",
       " 'disbelief',\n",
       " 'separated',\n",
       " 'nmetzler',\n",
       " 'nlwcw',\n",
       " 'corolla',\n",
       " 'fruittessly',\n",
       " 'are',\n",
       " 'nenclosures',\n",
       " 'construing',\n",
       " 'issuesnrcquqp',\n",
       " 'hoc',\n",
       " 'debbie',\n",
       " 'ndorothy',\n",
       " 'scorn',\n",
       " 'ceaelesiees',\n",
       " 'staimey',\n",
       " 'entere',\n",
       " 'threecase',\n",
       " 'prichard',\n",
       " 'wrinkle',\n",
       " 'fearing',\n",
       " 'npersisted',\n",
       " 'hernandez',\n",
       " 'nwbedichonalwuniessdneesiiy',\n",
       " 'sneaky',\n",
       " 'nexperis',\n",
       " 'nstrategies',\n",
       " 'toss',\n",
       " 'dimitri',\n",
       " 'nlatausha',\n",
       " 'iiiii',\n",
       " 'treatm',\n",
       " 'monnomm',\n",
       " 'inmeeteeiwieheeb',\n",
       " 'nwwwuswcasw',\n",
       " 'participati',\n",
       " 'success',\n",
       " 'sigida',\n",
       " 'nreached',\n",
       " 'housdjgi',\n",
       " 'spedry',\n",
       " 'education',\n",
       " 'categorized',\n",
       " 'nfranchisees',\n",
       " 'raising',\n",
       " 'enter',\n",
       " 'unrelated',\n",
       " 'nsssmvmlmmmooucrmam',\n",
       " 'namandmadmmm',\n",
       " 'sts',\n",
       " 'fihd',\n",
       " 'nfurfeiture',\n",
       " 'marketing',\n",
       " 'nqamlwnawe',\n",
       " 'hordes',\n",
       " 'method',\n",
       " 'oneok',\n",
       " 'nbene',\n",
       " 'punches',\n",
       " 'violent',\n",
       " 'furnish',\n",
       " 'gmcom',\n",
       " 'aoe',\n",
       " 'discqvery',\n",
       " 'slot',\n",
       " 'hundred',\n",
       " 'deming',\n",
       " 'abba',\n",
       " 'uvenile',\n",
       " 'ndybach',\n",
       " 'registration',\n",
       " 'marsha',\n",
       " 'nedmrd',\n",
       " 'cm',\n",
       " 'cartersville',\n",
       " 'physiological',\n",
       " 'generation',\n",
       " 'evade',\n",
       " 'sandeep',\n",
       " 'countess',\n",
       " 'duct',\n",
       " 'barbara',\n",
       " 'coerce',\n",
       " 'cemments',\n",
       " 'search',\n",
       " 'speakm',\n",
       " 'nonuexempt',\n",
       " 'patient',\n",
       " 'dining',\n",
       " 'offebruaqz',\n",
       " 'oee',\n",
       " 'precision',\n",
       " 'nvickie',\n",
       " 'comparator',\n",
       " 'brazilian',\n",
       " 'durin',\n",
       " 'justin',\n",
       " 'depth',\n",
       " 'ccnclude',\n",
       " 'mash',\n",
       " 'even',\n",
       " 'chill',\n",
       " 'nabbve',\n",
       " 'larkins',\n",
       " 'greensboro',\n",
       " 'umcnmmqspmuwcm',\n",
       " 'nyunmji',\n",
       " 'authurtzed',\n",
       " 'detennination',\n",
       " 'holding',\n",
       " 'unmnnliylargcnnmhsrufcizimsurdafmm',\n",
       " 'nashville',\n",
       " 'laborcase',\n",
       " 'shaving',\n",
       " 'nfollowws',\n",
       " 'not',\n",
       " 'nsatchell',\n",
       " 'ruin',\n",
       " 'planner',\n",
       " 'npharmace',\n",
       " 'uuilz',\n",
       " 'hms',\n",
       " 'alysia',\n",
       " 'ssell',\n",
       " 'nldlmj',\n",
       " 'ndiscriminates',\n",
       " 'sugarloaf',\n",
       " 'wilson',\n",
       " 'fomca',\n",
       " 'someonefs',\n",
       " 'fqewnf',\n",
       " 'ratherrtt',\n",
       " 'nassisting',\n",
       " 'fischer',\n",
       " 'moesh',\n",
       " 'officially',\n",
       " 'nonwdisabled',\n",
       " 'genitalia',\n",
       " 'genuine',\n",
       " 'symptom',\n",
       " 'environmental',\n",
       " 'detectives',\n",
       " 'ndmmn',\n",
       " 'saiary',\n",
       " 'lisa',\n",
       " 'painted',\n",
       " 'pesitien',\n",
       " 'verifiable',\n",
       " 'neil',\n",
       " 'neebc',\n",
       " 'attorriey',\n",
       " 'powrmlvdg',\n",
       " 'nbudgets',\n",
       " 'citizenofthiss',\n",
       " 'swmmmwwe',\n",
       " 'comussion',\n",
       " 'busk',\n",
       " 'mwr',\n",
       " 'light',\n",
       " 'harrilson',\n",
       " 'nsuspensions',\n",
       " 'nveri',\n",
       " 'pzaz',\n",
       " 'companywide',\n",
       " 'stanford',\n",
       " 'rmo',\n",
       " 'ndemonstrations',\n",
       " 'nalliances',\n",
       " 'taxi',\n",
       " 'couns',\n",
       " 'disturbing',\n",
       " 'nelba',\n",
       " 'entitled',\n",
       " 'strain',\n",
       " 'hire',\n",
       " 'ntriable',\n",
       " 'wmdmg',\n",
       " 'wns',\n",
       " 'wprgrwkn',\n",
       " 'andpending',\n",
       " 'resolving',\n",
       " 'nwgration',\n",
       " 'wigmhon',\n",
       " 'refocused',\n",
       " 'addree',\n",
       " 'princeal',\n",
       " 'braves',\n",
       " 'nome',\n",
       " 'nsupervisorlmanager',\n",
       " 'nmummw',\n",
       " 'lumen',\n",
       " 'dislodged',\n",
       " 'govemment',\n",
       " 'nifin',\n",
       " 'justifyng',\n",
       " 'marley',\n",
       " 'lsnotteheckonebox',\n",
       " 'keen',\n",
       " 'thang',\n",
       " 'rebuttal',\n",
       " 'meeta',\n",
       " 'suppressing',\n",
       " 'burnough',\n",
       " 'proposal',\n",
       " 'obhgagnons',\n",
       " 'preventioncase',\n",
       " 'hhst',\n",
       " 'ofcase',\n",
       " 'mans',\n",
       " 'ndrinks',\n",
       " 'prejudice',\n",
       " 'sameone',\n",
       " 'motivating',\n",
       " 'nacase',\n",
       " 'spoke',\n",
       " 'nconsistentiy',\n",
       " 'inquiry',\n",
       " 'dmsmr',\n",
       " 'esbestds',\n",
       " 'independently',\n",
       " 'taint',\n",
       " 'ccmplaint',\n",
       " 'nlassiter',\n",
       " 'ira',\n",
       " 'reaffirmed',\n",
       " 'rackei',\n",
       " 'gma',\n",
       " 'protecting',\n",
       " 'opposition',\n",
       " 'soft',\n",
       " 'nwom',\n",
       " 'magisi',\n",
       " 'plalntiff',\n",
       " 'chance',\n",
       " 'holloway',\n",
       " 'nentitles',\n",
       " 'accused',\n",
       " 'ruies',\n",
       " 'referred',\n",
       " 'relevant',\n",
       " 'lebanese',\n",
       " 'paper',\n",
       " 'alleging',\n",
       " 'marriage',\n",
       " 'outgfthe',\n",
       " 'ethnicity',\n",
       " 'pursuing',\n",
       " 'fry',\n",
       " 'ternlunder',\n",
       " 'multidib',\n",
       " 'ipad',\n",
       " 'chamber',\n",
       " 'july',\n",
       " 'nrosecommon',\n",
       " 'nkmmm',\n",
       " 'deimnee',\n",
       " 'africanuamei',\n",
       " 'alternatively',\n",
       " 'conscientious',\n",
       " 'nsaymon',\n",
       " 'practices',\n",
       " 'gluing',\n",
       " 'amendement',\n",
       " 'praise',\n",
       " 'say',\n",
       " 'schwartzer',\n",
       " 'wheel',\n",
       " 'wife',\n",
       " 'enjoining',\n",
       " 'labia',\n",
       " 'nmisclassified',\n",
       " 'discriminatian',\n",
       " 'confederate',\n",
       " 'unquali',\n",
       " 'exchanle',\n",
       " 'button',\n",
       " 'routinelycase',\n",
       " 'poles',\n",
       " 'unse',\n",
       " 'servers',\n",
       " 'deslsmn',\n",
       " 'learning',\n",
       " 'alters',\n",
       " 'sigh',\n",
       " 'pserda',\n",
       " 'licensed',\n",
       " 'cycles',\n",
       " 'nyum',\n",
       " 'defaultee',\n",
       " 'nunljr',\n",
       " 'ummuamwm',\n",
       " 'thatcover',\n",
       " 'rackffeer',\n",
       " 'wake',\n",
       " 'connecting',\n",
       " 'technicians',\n",
       " 'sinegal',\n",
       " 'ntransacts',\n",
       " 'nonmutual',\n",
       " 'nothee',\n",
       " 'nyancy',\n",
       " 'dominated',\n",
       " 'grandview',\n",
       " 'studies',\n",
       " 'unknewn',\n",
       " 'genital',\n",
       " 'shellae',\n",
       " 'constituted',\n",
       " 'ndekalb',\n",
       " 'rye',\n",
       " 'nmmmmmmmam',\n",
       " 'costen',\n",
       " 'appeujate',\n",
       " 'transfer',\n",
       " 'ing',\n",
       " 'released',\n",
       " 'prefer',\n",
       " 'ndhhs',\n",
       " 'neloisa',\n",
       " 'intentiena',\n",
       " 'excessively',\n",
       " 'gulf',\n",
       " 'mudnd',\n",
       " 'cit',\n",
       " 'relatedirefiled',\n",
       " 'position',\n",
       " 'amongcase',\n",
       " 'mentality',\n",
       " 'kellogg',\n",
       " 'un',\n",
       " 'wheeler',\n",
       " 'coworkers',\n",
       " 'betty',\n",
       " 'collectlve',\n",
       " 'footing',\n",
       " 'lay',\n",
       " 'hisham',\n",
       " 'exh',\n",
       " 'regulation',\n",
       " 'noncompliant',\n",
       " 'applies',\n",
       " 'appellate',\n",
       " 'eesc',\n",
       " 'cognitive',\n",
       " 'exam',\n",
       " 'hige',\n",
       " 'attendance',\n",
       " 'officials',\n",
       " 'perfermed',\n",
       " 'singh',\n",
       " 'imde',\n",
       " 'nattn',\n",
       " 'give',\n",
       " 'achon',\n",
       " 'folfowing',\n",
       " 'exas',\n",
       " 'nimpairments',\n",
       " 'sch',\n",
       " 'karen',\n",
       " 'nlamont',\n",
       " 'next',\n",
       " 'niqlwms',\n",
       " 'nadmonished',\n",
       " 'ofsiate',\n",
       " 'informally',\n",
       " 'stocker',\n",
       " 'meq',\n",
       " 'onlyif',\n",
       " 'gurillo',\n",
       " 'reimbursements',\n",
       " 'inceme',\n",
       " 'vaughn',\n",
       " 'departm',\n",
       " 'joints',\n",
       " 'summarycase',\n",
       " 'groping',\n",
       " 'pretty',\n",
       " 'warrantedcase',\n",
       " 'wmngfud',\n",
       " 'seemingly',\n",
       " 'royal',\n",
       " 'glacial',\n",
       " 'regulatory',\n",
       " 'bushess',\n",
       " 'correctly',\n",
       " 'courtroom',\n",
       " 'bona',\n",
       " 'ischer',\n",
       " 'aylor',\n",
       " 'probably',\n",
       " 'nrevealed',\n",
       " 'covmmzm',\n",
       " 'krumper',\n",
       " 'joe',\n",
       " 'pointed',\n",
       " 'tor',\n",
       " 'actiee',\n",
       " 'nmumsssmmm',\n",
       " 'nvvvvvvvvv',\n",
       " 'nburrows',\n",
       " 'ash',\n",
       " 'oakley',\n",
       " 'stiii',\n",
       " 'subscribed',\n",
       " 'ndeaat',\n",
       " 'village',\n",
       " 'run',\n",
       " 'caesar',\n",
       " 'qatavided',\n",
       " 'nmlyed',\n",
       " 'receivable',\n",
       " 'effrey',\n",
       " 'nikea',\n",
       " 'hast',\n",
       " 'overpayreent',\n",
       " 'representatives',\n",
       " 'nperforms',\n",
       " 'hycmmsal',\n",
       " 'beoc',\n",
       " 'presidential',\n",
       " 'interpretation',\n",
       " 'diverted',\n",
       " 'orge',\n",
       " 'miserable',\n",
       " 'conclusively',\n",
       " 'gratings',\n",
       " 'prohibiting',\n",
       " 'lawrenceville',\n",
       " 'foregoingy',\n",
       " 'contends',\n",
       " 'stump',\n",
       " 'persona',\n",
       " 'salesperson',\n",
       " 'cellco',\n",
       " 'transmission',\n",
       " 'ida',\n",
       " 'kc',\n",
       " 'nshefizadeh',\n",
       " 'mmmmmmmmmmmmmnumnmmnmsm',\n",
       " 'mmwmmmmrzg',\n",
       " 'adherence',\n",
       " 'notably',\n",
       " 'dishes',\n",
       " 'pleaded',\n",
       " 'klelvin',\n",
       " 'varying',\n",
       " 'income',\n",
       " 'nomissions',\n",
       " 'nplastics',\n",
       " 'reseekce',\n",
       " 'instrument',\n",
       " 'fiefd',\n",
       " 'diabetic',\n",
       " 'ejeclment',\n",
       " 'stereotyped',\n",
       " 'ngrabbing',\n",
       " 'type',\n",
       " 'so',\n",
       " 'abor',\n",
       " 'guidera',\n",
       " 'polite',\n",
       " 'nrecognizes',\n",
       " 'nnetice',\n",
       " 'unexpected',\n",
       " 'discriminaticn',\n",
       " 'ofdiscrimination',\n",
       " 'fringe',\n",
       " 'alam',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_uniqueTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A DATAFRAME OF THE UNIQUE WORDS AND SET THE WORDS AS THE INDEX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(List_uniqueTokens)\n",
    "df_set_index = df.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART I:  CREATE A DATAFRAME REPRESENTING EACH TEXT IN THE CATEGORY AND COUNT OF UNIQUE WORDS PRESENT IN EACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencyDist_legal_text(Dir_list, df_unique_words):\n",
    "    '''Purpose is to get the freq dist of words match in given text\n",
    "    Input1 = Directory list\n",
    "    Input2 = Dataframe whose index comprises the list of unique words\n",
    "    Ouput  = Dataframe whose index is a list of unique words and whose columns or data points is a percentage that \n",
    "             represents the presence of that words in the underlying texts. \n",
    "    '''\n",
    "\n",
    "    # Define the Initial Dataframe to be df_unique_words.  Then begin to append columns for each text. \n",
    "    Create_dist_freq_dataframe = df_unique_words\n",
    "    \n",
    "    # Loop over directory of files to obtain the text files. \n",
    "    Text_files = [file for file in Dir_list if '.txt' in file]\n",
    "    \n",
    "    # Create short list of text files for testing\n",
    "    Short_list = Text_files[3:6]   \n",
    "    \n",
    "    # Loop over files\n",
    "    \n",
    "    for file in Short_list:  # Change to Text_files when \n",
    "    \n",
    "        # List Catch Values (1/0)\n",
    "        List_match_count = []\n",
    "    \n",
    "        # Open & Read Text\n",
    "        Open_file = open(file, 'rb')\n",
    "        Read_file = Open_file.read()\n",
    "        Str_file = str(Read_file)\n",
    "    \n",
    "        # Run Text through cleaning pipeline       \n",
    "        Clean_tokenized_text = get_clean_text_using_text_clearning_pipeline(Str_file)\n",
    "    \n",
    "        # Loop over list of unique words and see if there is a match in the cleaned text. \n",
    "        for token in df_unique_words.index:\n",
    "            # If there is a match append 1 to the list. \n",
    "            if token in Clean_tokenized_text:\n",
    "                List_match_count.append(1)\n",
    "            # If there is no match, append 0\n",
    "            else:\n",
    "                List_match_count.append(0)\n",
    "        \n",
    "        # Create a column for each file and whose values is the list of matches (1/0)\n",
    "        Create_dist_freq_dataframe[file] = List_match_count\n",
    "    \n",
    "    return Create_dist_freq_dataframe\n",
    "    \n",
    "  \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_dist_words_concat_txt = get_frequencyDist_legal_text(Dir_list, df_set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get_dist_words_concat_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean_tokenized_text = get_frequencyDist_legal_text(Dir_list, df_set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART II:  IMPORT THE DATAFRAME OF ALL TEXT - EXPORT DATAFRAME W/ VALUES = % FOR THIS GROUP OF DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_dataframe_columns_calc_percentage(Dataframe):\n",
    "    '''\n",
    "    Input  = Dataframe that captured the matches for each text\n",
    "    Output = Dataframe that captures the % of matches for all text files\n",
    "    '''\n",
    "    Dict_freq_dist = {}\n",
    "    List_perct = []\n",
    "    \n",
    "    for row in Dataframe.index:\n",
    "        Sum = sum(Dataframe.loc[row])\n",
    "        Len = len(Dataframe.loc[row])\n",
    "        Perct = Sum / Len\n",
    "        Dict_freq_dist[row] = Perct      \n",
    "    \n",
    "    # Recreate Dataframe\n",
    "    \n",
    "    List_index = [1]\n",
    "    \n",
    "    df_perct = pd.DataFrame(Dict_freq_dist, index = List_index)\n",
    "    df_transpose = pd.DataFrame.transpose(df_perct)\n",
    "    df_transpose.columns = ['Complaints'] \n",
    "    df_sort = df_transpose.sort_values(by = 'Complaints', ascending = False)\n",
    "    \n",
    "    return df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc_per_all_docs(Get_dist_words_concat_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A PIPELINE FUNCTION THAT WILL IDENTIFY THE GROUPING OF THE DOCS TO PIPE INTO YOUR FUNCTIONS ABOVE AND THEN WRITES\n",
    "# THE GROUP DATAFRAME TO EXCEL. \n",
    "# AlSO CREATE A FILE TO CAPTURE ALL OF YOUR FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "Dir_list = os.listdir()\n",
    "\n",
    "\n",
    "Text_class_file = [file for file in Dir_list if 'Wang' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   FILE  NUM              CLASS  Unnamed: 3  \\\n",
      "0  GA_Northern_1_15-cv-04247-TWT_26.txt    2              Order         NaN   \n",
      "1  GA_Northern_1_15-cv-04247-TWT_32.txt    2              Order         NaN   \n",
      "2   GA_Northern_1_15-cv-04249-ODE_4.txt    5  Civil Cover Sheet         NaN   \n",
      "3    GA_Northern_1_15-cv-04258-AT_0.txt    1          Compliant         NaN   \n",
      "4    GA_Northern_1_15-cv-04260-CC_0.txt    1          Compliant         NaN   \n",
      "\n",
      "   Unnamed: 4    Unnamed: 5  \n",
      "0         NaN           NaN  \n",
      "1         NaN           NaN  \n",
      "2         1.0     Compliant  \n",
      "3         2.0         Order  \n",
      "4         3.0  Docket Sheet  \n"
     ]
    }
   ],
   "source": [
    "Classification_file = 'Text_Classification_Wang.xlsx'\n",
    "df_txt_class = pd.read_excel(Classification_file)\n",
    "print(df_txt_class.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         GA_Northern_1_15-cv-04247-TWT_26.txt\n",
       "1         GA_Northern_1_15-cv-04247-TWT_32.txt\n",
       "2          GA_Northern_1_15-cv-04249-ODE_4.txt\n",
       "3           GA_Northern_1_15-cv-04258-AT_0.txt\n",
       "4           GA_Northern_1_15-cv-04260-CC_0.txt\n",
       "5           GA_Northern_1_15-cv-04260-CC_8.txt\n",
       "6           GA_Northern_1_15-cv-04264-AT_0.txt\n",
       "7          GA_Northern_1_15-cv-04281-RWS_0.txt\n",
       "8          GA_Northern_1_15-cv-04284-WSD_0.txt\n",
       "9           GA_Northern_1_15-cv-04285-AT_0.txt\n",
       "10          GA_Northern_1_15-cv-04288-AT_0.txt\n",
       "11         GA_Northern_1_15-cv-04298-MHC_0.txt\n",
       "12         GA_Northern_1_15-cv-04303-ODE_0.txt\n",
       "13         GA_Northern_1_15-cv-04310-LMM_0.txt\n",
       "14         GA_Northern_1_15-cv-04324-MHC_0.txt\n",
       "15         GA_Northern_1_15-cv-04330-ODE_0.txt\n",
       "16         GA_Northern_1_15-cv-04353-SCJ_0.txt\n",
       "17     GA_Northern_1_15-cv-04379-MHC-AJB_0.txt\n",
       "18         GA_Northern_1_15-cv-04389-SCJ_0.txt\n",
       "19         GA_Northern_1_15-cv-04394-ODE_0.txt\n",
       "20        GA_Northern_1_15-cv-04394-ODE_66.txt\n",
       "21        GA_Northern_1_15-cv-04394-ODE_81.txt\n",
       "22         GA_Northern_1_15-cv-04401-TWT_0.txt\n",
       "23         GA_Northern_1_15-cv-04406-ELR_0.txt\n",
       "24         GA_Northern_1_15-cv-04419-TWT_0.txt\n",
       "25        GA_Northern_1_15-cv-04419-TWT_56.txt\n",
       "26        GA_Northern_1_15-cv-04419-TWT_70.txt\n",
       "27         GA_Northern_1_15-cv-04424-RWS_0.txt\n",
       "28          GA_Northern_1_15-cv-04446-AT_0.txt\n",
       "29         GA_Northern_1_15-cv-04446-AT_53.txt\n",
       "                        ...                   \n",
       "70    GA_Northern_1_16-cv-00072-TWT-AJB_72.txt\n",
       "71         GA_Northern_1_16-cv-00075-SCJ_0.txt\n",
       "72         GA_Northern_1_16-cv-00081-RWS_0.txt\n",
       "73        GA_Northern_1_16-cv-00081-RWS_45.txt\n",
       "74        GA_Northern_1_16-cv-00081-RWS_48.txt\n",
       "75        GA_Northern_1_16-cv-00081-RWS_51.txt\n",
       "76        GA_Northern_1_16-cv-00081-RWS_53.txt\n",
       "77        GA_Northern_1_16-cv-00081-RWS_61.txt\n",
       "78         GA_Northern_1_16-cv-00084-WSD_0.txt\n",
       "79         GA_Northern_1_16-cv-00085-SCJ_0.txt\n",
       "80         GA_Northern_1_16-cv-00113-SCJ_0.txt\n",
       "81         GA_Northern_1_16-cv-00118-WSD_0.txt\n",
       "82         GA_Northern_1_16-cv-00158-MHC_0.txt\n",
       "83         GA_Northern_1_16-cv-00162-WSD_3.txt\n",
       "84         GA_Northern_1_16-cv-00186-MHC_0.txt\n",
       "85         GA_Northern_1_16-cv-00187-TWT_0.txt\n",
       "86        GA_Northern_1_16-cv-00187-TWT_23.txt\n",
       "87        GA_Northern_1_16-cv-00187-TWT_39.txt\n",
       "88        GA_Northern_1_16-cv-00187-TWT_54.txt\n",
       "89    GA_Northern_1_16-cv-00200-RWS-JSA_26.txt\n",
       "90     GA_Northern_1_16-cv-00200-RWS-JSA_3.txt\n",
       "91    GA_Northern_1_16-cv-00200-RWS-JSA_33.txt\n",
       "92         GA_Northern_1_16-cv-00229-RWS_0.txt\n",
       "93         GA_Northern_1_16-cv-00244-WSD_0.txt\n",
       "94         GA_Northern_1_16-cv-00249-SCJ_0.txt\n",
       "95         GA_Northern_1_16-cv-00258-LMM_0.txt\n",
       "96         GA_Northern_1_16-cv-00262-CAP_0.txt\n",
       "97         GA_Northern_1_16-cv-00264-SCJ_0.txt\n",
       "98         GA_Northern_1_16-cv-00274-WSD_0.txt\n",
       "99         GA_Northern_1_16-cv-00281-SCJ_0.txt\n",
       "Name: FILE, Length: 100, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txt_class_narrow = df_txt_class.iloc[:,0]\n",
    "df_txt_class_narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt_class.groupby(['CLASS', 'FILE']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
