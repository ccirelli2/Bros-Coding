{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe purpose of this code is to classify legal text by type.\\n\\nTypes include:\\n1.) Complaint\\n2.) Order\\n3.) Summary judgement\\n4.) Cover sheet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Part II Legal Doc Classification \n",
    "\n",
    "Part I was the concatenating and cleaning of the Text docs. \n",
    "\n",
    "Part II\n",
    "\n",
    "The purpose of this code is to create a frequency distribution of words aggregated by legal document type.  The words \n",
    "in each file are compared to a unique set of words created from all word documents.   These frequency dist can then \n",
    "be used to classify text by looking for differences amongst the text. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODULES FOR THIS PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GitHub\\\\Bros-Coding-master\\\\Bros-Coding')\n",
    "import Module_Legal_Doc_Classification as mldc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TARGET DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note:    The target directory should be changed by the user to point to the directory within which they have saved their\n",
    "            text files'''\n",
    "\n",
    "os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "Dir_list = os.listdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A DATAFRAME OF THE UNIQUE WORDS AND SET THE WORDS AS THE INDEX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'Module_Legal_Doc_Classification' has no attribute 'get_dataframe_unique_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-322-68d401ea7581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataframe_unique_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmldc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataframe_unique_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList_uniqueTokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'Module_Legal_Doc_Classification' has no attribute 'get_dataframe_unique_tokens'"
     ]
    }
   ],
   "source": [
    "dataframe_unique_tokens = mldc.get_dataframe_unique_tokens(List_uniqueTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART I:  CREATE A DATAFRAME REPRESENTING EACH TEXT IN THE CATEGORY AND COUNT OF UNIQUE WORDS PRESENT IN EACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencyDist_legal_text(List_txt_files, df_unique_words):\n",
    "    '''Purpose is to get the freq dist of words match in given text\n",
    "    Input1 = A list of text files for one class.  An external loop will run to feed this function a list of files for each \n",
    "             class. \n",
    "    Input2 = Dataframe whose index comprises the list of unique words\n",
    "    Ouput  = Dataframe whose index is a list of unique words and whose columns or data points is a percentage that \n",
    "             represents the presence of that words in the underlying texts. \n",
    "    '''\n",
    "\n",
    "    # Define the Initial Dataframe to be df_unique_words.  Then redefine once process has been completed \n",
    "    Create_dist_freq_dataframe = df_unique_words\n",
    "    \n",
    "    for file in List_txt_files:\n",
    "    \n",
    "        # Open & Read Text\n",
    "        Open_file = open(file, 'rb')\n",
    "        Read_file = Open_file.read()\n",
    "        Str_file = str(Read_file)\n",
    "    \n",
    "        # Run Text through cleaning pipeline       \n",
    "        Clean_tokenized_text = mldc.get_clean_text_using_text_clearning_pipeline(Str_file)\n",
    "         \n",
    "        # List Catch Values (1/0)\n",
    "        List_match_count = []\n",
    "\n",
    "        # Loop over list of unique words and see if there is a match in the cleaned text. \n",
    "        for token in df_unique_words.index:\n",
    "            # If there is a match append 1 to the list. \n",
    "            if token in Clean_tokenized_text:\n",
    "                List_match_count.append(1)\n",
    "            # If there is no match, append 0\n",
    "            else:\n",
    "                List_match_count.append(0)\n",
    "        \n",
    "        # Create a column for each file and whose values is the list of matches (1/0)\n",
    "        Create_dist_freq_dataframe[file] = List_match_count\n",
    "    \n",
    "    return Create_dist_freq_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean_tokenized_text = get_frequencyDist_legal_text(Dir_list, df_set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART II:  IMPORT THE DATAFRAME OF ALL TEXT - EXPORT DATAFRAME W/ VALUES = % FOR THIS GROUP OF DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_dataframe_columns_calc_percentage(Class_name, Dataframe):\n",
    "    '''\n",
    "    Input  = Dataframe that captured the matches for each text\n",
    "    Output = Dataframe that captures the % of matches for all text files\n",
    "    '''\n",
    "    Dict_freq_dist = {}\n",
    "    List_perct = []\n",
    "    \n",
    "    for row in Dataframe.index:\n",
    "        Sum = sum(Dataframe.loc[row])\n",
    "        Len = len(Dataframe.loc[row])\n",
    "        Perct = Sum / Len\n",
    "        Dict_freq_dist[row] = Perct      \n",
    "    \n",
    "    # Recreate Dataframe\n",
    "    \n",
    "    List_index = [1]\n",
    "    \n",
    "    df_perct = pd.DataFrame(Dict_freq_dist, index = List_index)\n",
    "    df_transpose = pd.DataFrame.transpose(df_perct)\n",
    "    df_transpose.columns = [str(Class_name)] \n",
    "\n",
    "    return df_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION FOR FORM THE LIST OF TEXT BY GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list_by_group(Excel_doc):\n",
    "    '''\n",
    "    Input   = Excel Doc\n",
    "    Output  = List of Lists\n",
    "    '''\n",
    "    \n",
    "    # Create a Dictionary to capture class names & files\n",
    "    \n",
    "    Dict_classes_files = {}\n",
    "    \n",
    "    # Create Dataframe in memory\n",
    "    df = pd.read_excel(Excel_doc)\n",
    "    \n",
    "    # Limit to File & Class\n",
    "    df_file_class = df[['FILE','CLASS']]\n",
    "    \n",
    "    # Create list of classes\n",
    "    List_file_classes = list(set(df_file_class['CLASS']))\n",
    "\n",
    "    # Loop over class list to obtan\n",
    "    \n",
    "    for Class in List_file_classes:\n",
    "        Def_class = df_file_class['CLASS'] == Class \n",
    "        Limit_df = df_file_class[Def_class]\n",
    "        Dict_classes_files[Class] = Limit_df['FILE']\n",
    "        \n",
    "    return Dict_classes_files\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "Dir_list = os.listdir()\n",
    "Excel_file = 'Text_Classification_Wang.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_code_classify_texts_word_freq():\n",
    "    \n",
    "    # Import Packages\n",
    "    import nltk\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Import Personal Modules\n",
    "    os.chdir('C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GitHub\\\\Bros-Coding-master\\\\Bros-Coding')\n",
    "    import Module_Legal_Doc_Classification as mldc\n",
    "    \n",
    "    # Change to the target directory\n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    Dir_list = os.listdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    \n",
    "    # Create a list of unique tokens from the clean concatenated text file\n",
    "    List_uniqueTokens = mldc.get_list_uniqueTokens_from_cleaned_concat_text()\n",
    "    \n",
    "    # Create the base dataframe with the index as the set of unique tokens/words\n",
    "    dataframe_unique_tokens = get_dataframe_unique_tokens(List_uniqueTokens)\n",
    "    \n",
    "    # Define Excel doc where pre-classisified files are located\n",
    "    Excel_file = 'Text_Classification_Wang.xlsx'\n",
    "    \n",
    "    # Create a dictionary whose keys are the file classes and values the files for each group. \n",
    "    Dict_files_by_class = create_file_list_by_group(Excel_file)\n",
    "    \n",
    "    # Create a Master Dataframe to house final values\n",
    "    Master_dataframe = pd.DataFrame({}, index = list(dataframe_unique_tokens.index))\n",
    "    \n",
    "    # Loop over the dictionary keys\n",
    "    for Class in Dict_files_by_class.keys():\n",
    "        \n",
    "        # Create a list of the files\n",
    "        File_list = list(Dict_files_by_class[Class])\n",
    "        \n",
    "        # Feed each class list trough the word counter\n",
    "        df_single_class_word_count = get_frequencyDist_legal_text(File_list, dataframe_unique_tokens)\n",
    "        \n",
    "        # Merge individual text file columns into one percentage for the given class\n",
    "        df_merged_columns = Merge_dataframe_columns_calc_percentage(Class, df_single_class_word_count)\n",
    "        \n",
    "        # Merge individual dataframes into master dataframe\n",
    "        Master_dataframe[Class] = df_merged_columns[Class]\n",
    "        \n",
    "    # Return the Master Dataframe with the word % for all classes. \n",
    "    return Master_dataframe       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test = final_code_classify_texts_word_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final Reprot</th>\n",
       "      <th>Order</th>\n",
       "      <th>Compliant</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Civil Cover Sheet</th>\n",
       "      <th>Junk</th>\n",
       "      <th>Motion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scrutinized</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagby</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coim</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cam</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hammond</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Final Reprot     Order  Compliant   Summary  Civil Cover Sheet  \\\n",
       "scrutinized          0.00  0.000000   0.000000  0.000000           0.000000   \n",
       "bagby                0.25  0.030303   0.011236  0.011111           0.010309   \n",
       "coim                 0.00  0.000000   0.000000  0.000000           0.000000   \n",
       "cam                  0.00  0.000000   0.000000  0.000000           0.010309   \n",
       "hammond              0.00  0.000000   0.000000  0.000000           0.000000   \n",
       "\n",
       "                 Junk  Motion  \n",
       "scrutinized  0.000000    0.00  \n",
       "bagby        0.010204    0.01  \n",
       "coim         0.000000    0.00  \n",
       "cam          0.010204    0.01  \n",
       "hammond      0.000000    0.00  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Master Dataframe to Excel_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import Personal Modules\n",
    "os.chdir('C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GitHub\\\\Bros-Coding-master\\\\Bros-Coding')\n",
    "import Module_Legal_Doc_Classification as mldc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Master Dataframe to Excel \n",
    "os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "mldc.write_to_excel(Test, 'Master Word Dist Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
