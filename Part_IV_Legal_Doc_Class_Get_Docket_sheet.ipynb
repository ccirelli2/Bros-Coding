{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe purpose of this code is to classify legal text by type.\\n\\nTypes include:\\n1.) Complaint\\n2.) Order\\n3.) Summary judgement\\n4.) Cover sheet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Part III Attempt to Classify the Complaint Files'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Personal Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GitHub\\\\Bros-Coding-master\\\\Bros-Coding')\n",
    "import Module_Part_III_Legal_Doc_Classification as mldc_pIII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CLASSIFIER THAT FINDS DOCKET SHEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Docket_Sheet():\n",
    "    \n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    Dir_list = os.listdir()\n",
    "    List_text_files = [x for x in Dir_list if '.txt' in x]\n",
    "    \n",
    "    Dict = {}\n",
    "    \n",
    "    # Loop over list of text files\n",
    "    for file in List_text_files[4:]:\n",
    "        \n",
    "        #Open and Read them into memory. \n",
    "        Open = open(file, 'rb')\n",
    "        Read = Open.read()\n",
    "        Str = str(Read.lower())\n",
    "        \n",
    "        # Clean Text & Tokenize (Probably should slice first, but whatever)\n",
    "        Tokenize_text = nltk.word_tokenize(Str)\n",
    "        \n",
    "        # Take first 200 Tokens \n",
    "        Slice_of_bread = Tokenize_text[:600]\n",
    "        \n",
    "        # Create a list with the rankings\n",
    "        \n",
    "        if 'docket' in Slice_of_bread:\n",
    "            Dict[file] = 1\n",
    "        else:\n",
    "            Dict[file] = 0\n",
    "    \n",
    "    # Create Dataframe\n",
    "    \n",
    "    List_index = [1]\n",
    "    df = pd.DataFrame(Dict, index = List_index)\n",
    "    df_transpose = pd.DataFrame.transpose(df) \n",
    "    df_transpose.columns = ['Docket_Sheet = 1'] \n",
    "    \n",
    "    # Write to an Excel file\n",
    "    \n",
    "    mldc_pII.write_to_excel(df_transpose, 'Docket_Sheet_Classification_Output_lower')\n",
    "    \n",
    "    return df_transpose\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Get_Docket_Sheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCAT DOCKET SHEET FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    " os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "List_docket_sheet_files = pd.read_excel('Docket_Sheet_Classification_Output_lower.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Concatenated_text_file(List_docket_sheet_files, New_file_name):     \n",
    "    # Create new write file\n",
    "    New_File = open(str(New_file_name) + '.txt','w')\n",
    "    \n",
    "    # Create Loop Through List of Directories\n",
    "    for x in List_docket_sheet_files.iloc[:,0]:\n",
    "        File = open(x, 'rb')\n",
    "        Text = File.read()\n",
    "        \n",
    "        # Write files to new file\n",
    "        New_File.write(str(Text))\n",
    "        New_File.write('\\n')\n",
    "    # Close File\n",
    "    New_File.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "Concat_obj = create_Concatenated_text_file(List_docket_sheet_files, 'Docket_sheets_concat_file_Chris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SET FROM CONCAT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_from_concat_text():\n",
    "\n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    File = 'Docket_sheets_concat_file_Chris.txt'\n",
    "    Open = open(File, 'rb')\n",
    "    Read = Open.read()\n",
    "    Str = str(Read)\n",
    "    clean = mldc_pIII.get_clean_text_using_text_clearning_pipeline(Str)\n",
    "    Set = list(set(clean))\n",
    "    \n",
    "    return Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Docket_sheet_set = get_set_from_concat_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WordFreqDist_Docket_sheet_files(Set):\n",
    "    os.chdir(r'I:\\Legal Analytics Sprint-S18\\Team Folders\\Team Wang\\Files Converted to Txt')\n",
    "    List_docket_sheet_files = pd.read_excel('Docket_Sheet_Classification_Output_lower.xlsx')\n",
    "    \n",
    "    Dict = {}\n",
    "    \n",
    "    for x in List_docket_sheet_files.iloc[:,0]:\n",
    "        File = open(x, 'rb')\n",
    "        Text = File.read()\n",
    "        Str = str(Text)\n",
    "        clean = mldc_pII.get_clean_text_using_text_clearning_pipeline(Str)\n",
    "        \n",
    "        for token in clean:\n",
    "            if token in Set:\n",
    "                Dict.get(token, 0) + 1\n",
    "        \n",
    "    return Dict\n",
    "            \n",
    "'''maybe read the set into the dictionary keys first'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2746d7970b4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mFreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_WordFreqDist_Docket_sheet_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDocket_sheet_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-e0ce0d98f1c2>\u001b[0m in \u001b[0;36mget_WordFreqDist_Docket_sheet_files\u001b[1;34m(Set)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Freq = get_WordFreqDist_Docket_sheet_files(Docket_sheet_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
